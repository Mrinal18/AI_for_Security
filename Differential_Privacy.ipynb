{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential Privacy.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOHOIt5mh1czdHypmhY6BGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrinal18/AI_for_Security/blob/main/Differential_Privacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIWPtp5O1BA5"
      },
      "source": [
        "!pip install tensorflow-privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2DcXWzu4Qv4"
      },
      "source": [
        "\"\"\"\n",
        "1. You should train differentially private models on the CIFAR-100 dataset. \n",
        "2. You will use TensorFlow Privacy which provide DP implementations of standard optimizers, but you only need to pick one. \n",
        "3. The goal is to train a well performing model while keeping a small value for epsilon during training. \n",
        "4. You will use a fixed delta of 4 × 10−5. \n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6et2zT5orb"
      },
      "source": [
        "# load data\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
        "\n",
        "# normalize data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# split data into validation and training set\n",
        "validation_images = train_images[:5000]\n",
        "validation_labels = train_labels[:5000]\n",
        "train_images = train_images[5000:]\n",
        "train_labels = train_labels[5000:]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# create data generator\n",
        "data_generator = ImageDataGenerator(width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    horizontal_flip=True)\n",
        "\n",
        "# prepare iterator\n",
        "train_iterator = data_generator.flow(train_images, train_labels, batch_size=64)\n",
        "\n",
        "# prepare validation iterator\n",
        "test_iterator = data_generator.flow(test_images, test_labels, batch_size=64)\n",
        "\n",
        "# prepare validation iterator\n",
        "validation_iterator = data_generator.flow(validation_images, validation_labels, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUE8XTz8QXY_"
      },
      "source": [
        "\n",
        "# create optimizer Note: need to experitment with this optimizer wrt to report\n",
        "optimizer = DPGradientDescentGaussianOptimizer(\n",
        "    l2_norm_clip=1.0,\n",
        "    noise_multiplier=1.1,\n",
        "    num_microbatches=250,\n",
        "    learning_rate=0.15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q3KbsycQtux"
      },
      "source": [
        "#compute epsilon\n",
        "epsilon, delta = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=250, noise_multiplier=1.1, epochs=15, delta=1e-5)\n",
        "\n",
        "# train model\n",
        "print(\"Epsilon: \", epsilon)\n",
        "\n",
        "# train model\n",
        "model.fit(train_iterator,\n",
        "          epochs=100,\n",
        "          validation_data=validation_iterator,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# evaluate model\n",
        "test_loss, test_acc = model.evaluate(test_iterator, verbose=2)\n",
        "\n",
        "# print results\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# save model\n",
        "model.save('cifar100_model.h5')\n",
        "# # define loss function\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# # define metrics\n",
        "# metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "# # define privacy budget\n",
        "# epsilon = math.exp(math.log(1.25) / 10)\n",
        "\n",
        "# # define number of epochs\n",
        "# epochs = 10\n",
        "\n",
        "# # train model\n",
        "# model.fit(train_iterator,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=validation_iterator,\n",
        "#           validation_steps=1,\n",
        "#           verbose=1)\n",
        "\n",
        "# # evaluate model\n",
        "# model.evaluate(test_iterator, verbose=2)\n",
        "\n",
        "# # save model\n",
        "# model.save('cifar100_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0XZUfjqQum-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}